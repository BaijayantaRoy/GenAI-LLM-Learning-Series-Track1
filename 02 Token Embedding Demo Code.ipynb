{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37449635-eef2-47fb-91b8-8e81ffd78f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download Work of Rabindranath Tagore and Preprocess Text\n",
    "# Libraries for downloading and preprocessing\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_and_preprocess_text(url, local_file):\n",
    "    \"\"\"\n",
    "    Download text from a URL, save it locally, and preprocess it by extracting plain text from HTML.\n",
    "    Args:\n",
    "        url (str): URL to download the text from.\n",
    "        local_file (str): Local file path to save the downloaded content.\n",
    "    Returns:\n",
    "        str: Preprocessed plain text.\n",
    "    \"\"\"\n",
    "    # Check if the file already exists locally to avoid redundant downloads\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Downloading text file...\")\n",
    "        response = requests.get(url)\n",
    "        with open(local_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Text file downloaded and saved as {local_file}.\")\n",
    "    else:\n",
    "        print(f\"Text file {local_file} already exists.\")\n",
    "\n",
    "    # Read the HTML file and extract plain text using BeautifulSoup\n",
    "    with open(local_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "    # Sample sentence for demonstration\n",
    "    sample_sentence = \"She plays the piano well.\"\n",
    "    print(f\"\\nSample Sentence: {sample_sentence}\")\n",
    "\n",
    "    # Step 1: Download and preprocess text dataset\n",
    "    print(\"\\n=== Step 1: Downloading and Preprocessing Text ===\")\n",
    "    url = \"https://www.gutenberg.org/files/33525/33525-h/33525-h.htm\"\n",
    "    local_file = \"gutenberg_text.html\"\n",
    "    text = download_and_preprocess_text(url, local_file)\n",
    "    print(f\"Sample of Preprocessed Text:\\n{text[:1000]}...\")\n",
    "\n",
    "# Step 2: Train BPE Tokenizer and Tokenize Text\n",
    "# Libraries for tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def train_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Train a Byte Pair Encoding (BPE) tokenizer on the given text and tokenize the text.\n",
    "    BPE is a subword tokenization method that merges frequent character pairs.\n",
    "    Args:\n",
    "        text (str): Input text to train the tokenizer on.\n",
    "    Returns:\n",
    "        tuple: (tokenizer, tokens) - Trained tokenizer and list of tokens.\n",
    "    \"\"\"\n",
    "    # Initialize a BPE tokenizer with an unknown token for out-of-vocabulary words\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(vocab_size=500, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Split text into sentences for training\n",
    "    sentences = text.splitlines()\n",
    "    tokenizer.train_from_iterator(sentences, trainer)\n",
    "\n",
    "    # Tokenize the full text\n",
    "    tokens = tokenizer.encode(text).tokens\n",
    "    print(f\"\\nTotal Tokens Processed from Dataset: {len(tokens)}\")\n",
    "    return tokenizer, tokens\n",
    "\n",
    "    # Execute Step 2: Train BPE tokenizer and tokenize text\n",
    "    print(\"\\n=== Step 2: Training BPE Tokenizer and Tokenizing Text ===\")\n",
    "    tokenizer, tokens = train_tokenizer(text)\n",
    "\n",
    "    # Execute Step 2: demonstrate with the sample sentence\n",
    "    sample_tokens = tokenizer.encode(sample_sentence).tokens\n",
    "    print(f\"Sample Sentence Tokens: {sample_tokens}\")\n",
    "\n",
    "# Step 3: Download and Load GloVe Embeddings for Static Token Embeddings\n",
    "def download_glove_embeddings():\n",
    "    \"\"\"\n",
    "    Download GloVe embeddings if not already present locally.\n",
    "    GloVe embeddings are pre-trained static embeddings used for token representation.\n",
    "    \"\"\"\n",
    "    glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    glove_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "    if not os.path.exists(glove_file):\n",
    "        print(\"GloVe file not found locally, downloading...\")\n",
    "        glove_zip = \"glove.6B.zip\"\n",
    "        response = requests.get(glove_url)\n",
    "        with open(glove_zip, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extract(\"glove.6B.50d.txt\")\n",
    "        print(\"GloVe file downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"GloVe file already exists locally.\")\n",
    "\n",
    "# Import numpy for numerical operations (used in embedding loading)\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(glove_file):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file into a dictionary mapping tokens to their embeddings.\n",
    "    Args:\n",
    "        glove_file (str): Path to the GloVe embedding file.\n",
    "    Returns:\n",
    "        dict: Dictionary mapping tokens to their GloVe embeddings.\n",
    "    \"\"\"\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]  # First element is the token\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # Rest are embedding values\n",
    "            glove_embeddings[word] = vector\n",
    "    return glove_embeddings\n",
    "\n",
    "    # Step 3: Download and load GloVe embeddings for static token embeddings\n",
    "    print(\"\\n=== Step 3: Downloading and Loading GloVe Embeddings ===\")\n",
    "    download_glove_embeddings()\n",
    "    glove_embeddings = load_glove_embeddings(\"glove.6B.50d.txt\")\n",
    "    print(f\"Total GloVe Embeddings Loaded: {len(glove_embeddings)}\")\n",
    "\n",
    "# Step 4: Convert Tokens to Static GloVe Embeddings\n",
    "def get_embedding(token, glove_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Fetch the GloVe embedding for a token, or return a random embedding if not found.\n",
    "    Args:\n",
    "        token (str): Token to look up.\n",
    "        glove_embeddings (dict): Dictionary of GloVe embeddings.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "    Returns:\n",
    "        np.ndarray: Embedding vector for the token.\n",
    "    \"\"\"\n",
    "    return glove_embeddings.get(token.lower(), np.random.rand(embedding_dim))\n",
    "\n",
    "def convert_tokens_to_embeddings(tokens, glove_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their corresponding GloVe embeddings.\n",
    "    Args:\n",
    "        tokens (list): List of tokens.\n",
    "        glove_embeddings (dict): Dictionary of GloVe embeddings.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "    Returns:\n",
    "        list: List of embeddings for the tokens.\n",
    "    \"\"\"\n",
    "    embeddings = [get_embedding(token, glove_embeddings, embedding_dim) for token in tokens]\n",
    "    return embeddings\n",
    "\n",
    "    # Execution Step 4: Convert tokens to static GloVe embeddings\n",
    "    print(\"\\n=== Step 4: Converting Tokens to Static GloVe Embeddings ===\")\n",
    "\n",
    "    # Define constants for training\n",
    "    CONTEXT_LENGTH = 10  # Number of tokens per training sequence\n",
    "    BATCH_SIZE = 4  # Mini-batch size for processing\n",
    "    EMBEDDING_DIM = 50  # GloVe 50-dimensional embeddings\n",
    "\n",
    "    # First, demonstrate with the sample sentence\n",
    "    sample_tokens = tokenizer.encode(sample_sentence).tokens\n",
    "    print(f\"Sample Sentence Tokens: {sample_tokens}\")\n",
    "\n",
    "    static_sample_embeddings = convert_tokens_to_embeddings(sample_tokens, glove_embeddings, EMBEDDING_DIM)\n",
    "    print(\"Static Embeddings for Sample Sentence:\")\n",
    "    for token, emb in zip(sample_tokens, static_sample_embeddings):\n",
    "        print(f\"Token: {token}, Static Embedding: {emb[:6]} ...\")  # Display first 6 values in the 50 dimension\n",
    "\n",
    "    # Execution Step 4:  Apply to the dataset\n",
    "    static_token_embeddings = convert_tokens_to_embeddings(tokens, glove_embeddings, EMBEDDING_DIM)\n",
    "    print(f\"\\nStatic Embeddings for Dataset (First 5 Tokens):\")\n",
    "    for token, emb in zip(tokens[:5], static_token_embeddings[:5]):\n",
    "        print(f\"Token: {token}, Static Embedding: {emb[:6]} ...\")\n",
    "\n",
    "# Step 5: Add Positional Embeddings\n",
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Compute positional encodings using sine and cosine functions for each position in a sequence.\n",
    "    Positional encodings add order information to embeddings.\n",
    "    Args:\n",
    "        max_len (int): Maximum sequence length.\n",
    "        d_model (int): Embedding dimensionality.\n",
    "    Returns:\n",
    "        np.ndarray: Positional encoding matrix of shape (max_len, d_model).\n",
    "    \"\"\"\n",
    "    PE = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    return PE\n",
    "\n",
    "    # Execution Step 5: Add positional embeddings to create combined embeddings\n",
    "    print(\"\\n=== Step 5: Adding Positional Embeddings ===\")\n",
    "    # First, demonstrate with the sample sentence\n",
    "    position_embeddings_sample = positional_encoding(len(sample_tokens), EMBEDDING_DIM)\n",
    "    print(\"Positional Embeddings for Sample Sentence:\")\n",
    "    for pos, emb in enumerate(position_embeddings_sample):\n",
    "        print(f\"Position {pos}: {emb[:5]} ...\")\n",
    "\n",
    "# Step 6: Add Positional Embeddings with Static embedding to create Combined Embedding\n",
    "def add_positional_embeddings(token_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Add positional embeddings to token embeddings to create combined embeddings.\n",
    "    Args:\n",
    "        token_embeddings (list): List of token embeddings.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "    Returns:\n",
    "        list: List of combined embeddings (token + positional).\n",
    "    \"\"\"\n",
    "    position_embeddings = positional_encoding(len(token_embeddings), embedding_dim)\n",
    "    return [token_embeddings[i] + position_embeddings[i] for i in range(len(token_embeddings))]\n",
    "\n",
    "    # Execution Step 6: Add Positional Embeddings with Static embedding to create Combined Embedding\n",
    "    combined_sample_embeddings = add_positional_embeddings(static_sample_embeddings, EMBEDDING_DIM)\n",
    "    print(\"\\nCombined Embeddings (Static + Positional) for Sample Sentence:\")\n",
    "    for token, emb in zip(sample_tokens, combined_sample_embeddings):\n",
    "        print(f\"Token: {token}, Combined Embedding: {emb[:5]} ...\")\n",
    "\n",
    "    # Apply to the dataset (Combined Embeddings)\n",
    "    combined_embeddings = add_positional_embeddings(static_token_embeddings, EMBEDDING_DIM)\n",
    "    print(f\"\\nCombined Embeddings for Dataset (First 5 Tokens):\")\n",
    "    for token, emb in zip(tokens[:5], combined_embeddings[:5]):\n",
    "        print(f\"Token: {token}, Combined Embedding: {emb[:5]} ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d914101-e718-4da2-9a3e-a26076d216e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Sentence: She plays the piano well.\n",
      "\n",
      "=== Step 1: Downloading and Preprocessing Text ===\n",
      "Text file gutenberg_text.html already exists.\n",
      "Sample of Preprocessed Text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  The Project Gutenberg eBook of Stories from Tagore, by Rabindranath Tagore.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Project Gutenberg EBook of Stories from Tagore, by Rabindranath Tagore\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Stories from Tagore\n",
      "\n",
      "Author: Rabindranath Tagore\n",
      "\n",
      "Release Date: August 24, 2010 [EBook #33525]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: ISO-8859-1\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK STORIES FROM TAGORE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Audrey Longhurst, Asad Razzaki and the Online\n",
      "Distributed Proofreading Team at http://www.pgdp.net\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transcriber's Notes:\n",
      "Variations in spelling and hyphenation have been retained as\n",
      "in the original.\n",
      "Words listed in the 'Words to be Studied' sections are linked in\n",
      "the text like this. Click on the word to\n",
      "see the explanation.\n",
      "A few typog...\n",
      "\n",
      "=== Step 2: Training BPE Tokenizer and Tokenizing Text ===\n",
      "\n",
      "Total Tokens Processed from Dataset: 109604\n",
      "Sample Sentence Tokens: ['She', 'pl', 'ay', 's', 'the', 'p', 'i', 'an', 'o', 'we', 'll', '.']\n",
      "\n",
      "=== Step 3: Downloading and Loading GloVe Embeddings ===\n",
      "GloVe file already exists locally.\n",
      "Total GloVe Embeddings Loaded: 400000\n",
      "\n",
      "=== Step 4: Converting Tokens to Static GloVe Embeddings ===\n",
      "Static Embeddings for Sample Sentence:\n",
      "Token: She, Static Embedding: [ 0.060382  0.37821  -0.75142  -0.72159   0.58648   0.79126 ] ...\n",
      "Token: pl, Static Embedding: [-0.61566   0.21862   0.11926   0.28895  -0.10984  -0.041478] ...\n",
      "Token: ay, Static Embedding: [-0.24646  0.90635  0.97733 -0.62238 -0.6093  -0.64538] ...\n",
      "Token: s, Static Embedding: [-0.15234  0.98085  1.0065   0.97812  0.6628   0.11185] ...\n",
      "Token: the, Static Embedding: [ 0.418     0.24968  -0.41242   0.1217    0.34527  -0.044457] ...\n",
      "Token: p, Static Embedding: [-0.35559  1.2386   1.4348   1.0447   1.0335   0.10445] ...\n",
      "Token: i, Static Embedding: [ 0.11891   0.15255  -0.082073 -0.74144   0.75917  -0.48328 ] ...\n",
      "Token: an, Static Embedding: [ 0.36143   0.58615  -0.23718   0.079656  0.80192   0.49919 ] ...\n",
      "Token: o, Static Embedding: [-0.043861  1.3183   -0.03715   0.85478   0.12212  -0.77384 ] ...\n",
      "Token: we, Static Embedding: [ 0.57387  -0.32729   0.070521 -0.4198    0.862    -0.80001 ] ...\n",
      "Token: ll, Static Embedding: [-0.75747  0.36238  0.42209 -0.33792 -0.20081 -0.42729] ...\n",
      "Token: ., Static Embedding: [ 0.15164  0.30177 -0.16763  0.17684  0.31719  0.33973] ...\n",
      "\n",
      "Static Embeddings for Dataset (First 5 Tokens):\n",
      "Token: The, Static Embedding: [ 0.418     0.24968  -0.41242   0.1217    0.34527  -0.044457] ...\n",
      "Token: Project, Static Embedding: [ 1.1603    0.65813   0.3452    0.33018  -0.69697   0.098741] ...\n",
      "Token: Gutenberg, Static Embedding: [ 0.062151 -0.32266   0.34117  -1.0133   -0.49896   0.026478] ...\n",
      "Token: e, Static Embedding: [ 0.73833  0.65451  1.0873   0.86066 -0.4834  -0.9825 ] ...\n",
      "Token: B, Static Embedding: [-0.30116  0.33204  0.15823  0.90937  0.81527  0.57844] ...\n",
      "\n",
      "=== Step 5: Generating Positional Embeddings ===\n",
      "Positional Embeddings for Sample Sentence:\n",
      "Position 0: [0. 1. 0. 1. 0.] ...\n",
      "Position 1: [0.84147098 0.54030231 0.63794824 0.77007924 0.46056364] ...\n",
      "Position 2: [ 0.90929743 -0.41614684  0.9825414   0.18604408  0.81761716] ...\n",
      "Position 3: [ 0.14112001 -0.9899925   0.87532123 -0.48354188  0.99091397] ...\n",
      "Position 4: [-0.7568025  -0.65364362  0.36559202 -0.9307752   0.94150621] ...\n",
      "Position 5: [-0.95892427  0.28366219 -0.31225158 -0.94999945  0.6804981 ] ...\n",
      "Position 6: [-0.2794155   0.96017029 -0.84650894 -0.5323745   0.26655034] ...\n",
      "Position 7: [ 0.6569866   0.75390225 -0.99150634  0.13005834 -0.20730371] ...\n",
      "Position 8: [ 0.98935825 -0.14550003 -0.68056797  0.73268496 -0.63456696] ...\n",
      "Position 9: [ 0.41211849 -0.91113026 -0.05667618  0.99839261 -0.91921342] ...\n",
      "Position 10: [-0.54402111 -0.83907153  0.59327766  0.8049979  -0.99726976] ...\n",
      "Position 11: [-0.99999021  0.0044257   0.97041781  0.24143173 -0.85119308] ...\n",
      "\n",
      "Positional Embeddings for Dataset (First 5 Positions):\n",
      "Position 0: [0. 1. 0. 1. 0.] ...\n",
      "Position 1: [0.84147098 0.54030231 0.63794824 0.77007924 0.46056364] ...\n",
      "Position 2: [ 0.90929743 -0.41614684  0.9825414   0.18604408  0.81761716] ...\n",
      "Position 3: [ 0.14112001 -0.9899925   0.87532123 -0.48354188  0.99091397] ...\n",
      "Position 4: [-0.7568025  -0.65364362  0.36559202 -0.9307752   0.94150621] ...\n",
      "\n",
      "=== Step 6: Adding Positional Embeddings to Create Combined Embeddings ===\n",
      "\n",
      "Combined Embeddings (Static + Positional) for Sample Sentence:\n",
      "Token: She, Combined Embedding: [ 0.060382    1.37821001 -0.75142002  0.27841002  0.58648002] ...\n",
      "Token: pl, Combined Embedding: [0.22581097 0.75892231 0.75720824 1.05902924 0.35072364] ...\n",
      "Token: ay, Combined Embedding: [ 0.66283742  0.49020318  1.95987143 -0.43633594  0.20831714] ...\n",
      "Token: s, Combined Embedding: [-0.01121999 -0.00914252  1.88182123  0.49457815  1.65371399] ...\n",
      "Token: the, Combined Embedding: [-0.33880248 -0.40396362 -0.04682799 -0.80907521  1.28677622] ...\n",
      "Token: p, Combined Embedding: [-1.31451426  1.5222622   1.12254845  0.09470058  1.71399806] ...\n",
      "Token: i, Combined Embedding: [-0.1605055   1.11272028 -0.92858194 -1.2738145   1.02572033] ...\n",
      "Token: an, Combined Embedding: [ 1.01841659  1.34005224 -1.22868634  0.20971433  0.59461628] ...\n",
      "Token: o, Combined Embedding: [ 0.94549724  1.17279997 -0.71771796  1.58746497 -0.51244696] ...\n",
      "Token: we, Combined Embedding: [ 0.98598849 -1.23842026  0.01384481  0.5785926  -0.05721343] ...\n",
      "Token: ll, Combined Embedding: [-1.30149112 -0.47669153  1.01536766  0.46707789 -1.19807976] ...\n",
      "Token: ., Combined Embedding: [-0.84835021  0.3061957   0.80278781  0.41827174 -0.53400309] ...\n",
      "\n",
      "Combined Embeddings for Dataset (First 5 Tokens):\n",
      "Token: The, Combined Embedding: [ 0.41800001  1.24968    -0.41242     1.1217      0.34527001] ...\n",
      "Token: Project, Combined Embedding: [ 2.001771    1.1984323   0.98314825  1.10025923 -0.23640634] ...\n",
      "Token: Gutenberg, Combined Embedding: [ 0.97144843 -0.73880684  1.32371141 -0.82725586  0.31865717] ...\n",
      "Token: e, Combined Embedding: [ 0.87945001 -0.33548248  1.96262117  0.37711814  0.50751398] ...\n",
      "Token: B, Combined Embedding: [-1.0579625  -0.32160361  0.52382202 -0.0214052   1.75677622] ...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download Work of Rabindranath Tagore and Preprocess Text\n",
    "# Libraries for downloading and preprocessing\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_and_preprocess_text(url, local_file):\n",
    "    \"\"\"\n",
    "    Download text from a URL, save it locally, and preprocess it by extracting plain text from HTML.\n",
    "    This step prepares the raw text dataset for further processing like tokenization.\n",
    "    Args:\n",
    "        url (str): URL to download the text from (e.g., Project Gutenberg).\n",
    "        local_file (str): Local file path to save the downloaded content.\n",
    "    Returns:\n",
    "        str: Preprocessed plain text extracted from the HTML.\n",
    "    \"\"\"\n",
    "    # Check if the file already exists locally to avoid redundant downloads\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Downloading text file...\")\n",
    "        response = requests.get(url)\n",
    "        with open(local_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Text file downloaded and saved as {local_file}.\")\n",
    "    else:\n",
    "        print(f\"Text file {local_file} already exists.\")\n",
    "\n",
    "    # Read the HTML file and extract plain text using BeautifulSoup\n",
    "    with open(local_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "# Step 2: Train BPE Tokenizer and Tokenize Text\n",
    "# Libraries for tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def train_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Train a Byte Pair Encoding (BPE) tokenizer on the given text and tokenize the text.\n",
    "    BPE is a subword tokenization method that merges frequent character pairs to create a vocabulary.\n",
    "    This step converts raw text into a list of tokens for embedding generation.\n",
    "    Args:\n",
    "        text (str): Input text to train the tokenizer on.\n",
    "    Returns:\n",
    "        tuple: (tokenizer, tokens) - Trained tokenizer and list of tokens.\n",
    "    \"\"\"\n",
    "    # Initialize a BPE tokenizer with an unknown token for out-of-vocabulary words\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(vocab_size=500, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Split text into sentences for training the tokenizer\n",
    "    sentences = text.splitlines()\n",
    "    tokenizer.train_from_iterator(sentences, trainer)\n",
    "\n",
    "    # Tokenize the full text into a list of tokens\n",
    "    tokens = tokenizer.encode(text).tokens\n",
    "    print(f\"\\nTotal Tokens Processed from Dataset: {len(tokens)}\")\n",
    "    return tokenizer, tokens\n",
    "\n",
    "# Step 3: Download and Load GloVe Embeddings for Static Token Embeddings\n",
    "# Import numpy for numerical operations (used in embedding loading)\n",
    "import numpy as np\n",
    "\n",
    "def download_glove_embeddings():\n",
    "    \"\"\"\n",
    "    Download GloVe embeddings if not already present locally.\n",
    "    GloVe embeddings are pre-trained static embeddings used for token representation.\n",
    "    These embeddings map words to fixed vectors in a high-dimensional space capturing semantic meaning.\n",
    "    \"\"\"\n",
    "    glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    glove_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "    if not os.path.exists(glove_file):\n",
    "        print(\"GloVe file not found locally, downloading...\")\n",
    "        glove_zip = \"glove.6B.zip\"\n",
    "        response = requests.get(glove_url)\n",
    "        with open(glove_zip, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extract(\"glove.6B.50d.txt\")\n",
    "        print(\"GloVe file downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"GloVe file already exists locally.\")\n",
    "\n",
    "def load_glove_embeddings(glove_file):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file into a dictionary mapping tokens to their embeddings.\n",
    "    Args:\n",
    "        glove_file (str): Path to the GloVe embedding file (e.g., glove.6B.50d.txt).\n",
    "    Returns:\n",
    "        dict: Dictionary mapping tokens to their GloVe embeddings (e.g., \"cat\" → [0.123, -0.456, ...]).\n",
    "    \"\"\"\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]  # First element is the token\n",
    "            vector = np.array(values[1:], dtype=np.float32)  # Rest are embedding values\n",
    "            glove_embeddings[word] = vector\n",
    "    return glove_embeddings\n",
    "\n",
    "# Step 4: Convert Tokens to Static GloVe Embeddings\n",
    "def get_embedding(token, glove_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Fetch the GloVe embedding for a token, or return a random embedding if not found.\n",
    "    Args:\n",
    "        token (str): Token to look up (e.g., \"she\").\n",
    "        glove_embeddings (dict): Dictionary of GloVe embeddings.\n",
    "        embedding_dim (int): Dimensionality of the embeddings (e.g., 50 for GloVe 50D).\n",
    "    Returns:\n",
    "        np.ndarray: Embedding vector for the token (e.g., [0.123, -0.456, ...]).\n",
    "    \"\"\"\n",
    "    # Return the GloVe embedding if the token exists, otherwise generate a random embedding\n",
    "    return glove_embeddings.get(token.lower(), np.random.rand(embedding_dim))\n",
    "\n",
    "def convert_tokens_to_embeddings(tokens, glove_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their corresponding GloVe embeddings.\n",
    "    Args:\n",
    "        tokens (list): List of tokens (e.g., [\"She\", \"plays\", ...]).\n",
    "        glove_embeddings (dict): Dictionary of GloVe embeddings.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "    Returns:\n",
    "        list: List of embeddings for the tokens (e.g., [[0.123, -0.456, ...], ...]).\n",
    "    \"\"\"\n",
    "    embeddings = [get_embedding(token, glove_embeddings, embedding_dim) for token in tokens]\n",
    "    return embeddings\n",
    "\n",
    "# Step 5: Generate Positional Embeddings\n",
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Compute positional encodings using sine and cosine functions for each position in a sequence.\n",
    "    Positional encodings add order information to embeddings, ensuring the model understands token positions.\n",
    "    Args:\n",
    "        max_len (int): Maximum sequence length (e.g., number of tokens in the sequence).\n",
    "        d_model (int): Embedding dimensionality (e.g., 50 to match GloVe).\n",
    "    Returns:\n",
    "        np.ndarray: Positional encoding matrix of shape (max_len, d_model).\n",
    "    \"\"\"\n",
    "    # Initialize a matrix of zeros for positional encodings\n",
    "    PE = np.zeros((max_len, d_model))\n",
    "    # Compute sine and cosine values for each position and dimension\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    return PE\n",
    "\n",
    "# Step 6: Add Positional Embeddings to Static Embeddings to Create Combined Embeddings\n",
    "def add_positional_embeddings(token_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Add positional embeddings to token embeddings to create combined embeddings.\n",
    "    Combined embeddings encode both semantic meaning (static embeddings) and positional information.\n",
    "    Args:\n",
    "        token_embeddings (list): List of token embeddings (e.g., GloVe embeddings).\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "    Returns:\n",
    "        list: List of combined embeddings (token + positional).\n",
    "    \"\"\"\n",
    "    # Generate positional embeddings for the sequence length\n",
    "    position_embeddings = positional_encoding(len(token_embeddings), embedding_dim)\n",
    "    # Add positional embeddings to static embeddings element-wise\n",
    "    return [token_embeddings[i] + position_embeddings[i] for i in range(len(token_embeddings))]\n",
    "\n",
    "# Main execution with detailed steps\n",
    "if __name__ == \"__main__\":\n",
    "    # Define constants for training\n",
    "    CONTEXT_LENGTH = 10  # Number of tokens per training sequence (used for context window in later steps)\n",
    "    BATCH_SIZE = 4  # Mini-batch size for processing (used in batch processing)\n",
    "    EMBEDDING_DIM = 50  # GloVe 50-dimensional embeddings\n",
    "\n",
    "    # Sample sentence for demonstration\n",
    "    sample_sentence = \"She plays the piano well.\"\n",
    "    print(f\"\\nSample Sentence: {sample_sentence}\")\n",
    "\n",
    "    # Step 1: Download and preprocess text dataset\n",
    "    print(\"\\n=== Step 1: Downloading and Preprocessing Text ===\")\n",
    "    # Define the URL and local file path for downloading Rabindranath Tagore's work\n",
    "    url = \"https://www.gutenberg.org/files/33525/33525-h/33525-h.htm\"\n",
    "    local_file = \"gutenberg_text.html\"\n",
    "    # Download and preprocess the text to extract plain text\n",
    "    text = download_and_preprocess_text(url, local_file)\n",
    "    print(f\"Sample of Preprocessed Text:\\n{text[:1000]}...\")\n",
    "\n",
    "    # Step 2: Train BPE tokenizer and tokenize text\n",
    "    print(\"\\n=== Step 2: Training BPE Tokenizer and Tokenizing Text ===\")\n",
    "    # Train the tokenizer on the dataset and tokenize the entire text\n",
    "    tokenizer, tokens = train_tokenizer(text)\n",
    "\n",
    "    # Demonstrate tokenization with the sample sentence\n",
    "    sample_tokens = tokenizer.encode(sample_sentence).tokens\n",
    "    print(f\"Sample Sentence Tokens: {sample_tokens}\")\n",
    "\n",
    "    # Step 3: Download and load GloVe embeddings for static token embeddings\n",
    "    print(\"\\n=== Step 3: Downloading and Loading GloVe Embeddings ===\")\n",
    "    # Download GloVe embeddings if not present and load them into a dictionary\n",
    "    download_glove_embeddings()\n",
    "    glove_embeddings = load_glove_embeddings(\"glove.6B.50d.txt\")\n",
    "    print(f\"Total GloVe Embeddings Loaded: {len(glove_embeddings)}\")\n",
    "\n",
    "    # Step 4: Convert tokens to static GloVe embeddings\n",
    "    print(\"\\n=== Step 4: Converting Tokens to Static GloVe Embeddings ===\")\n",
    "    # First, demonstrate with the sample sentence\n",
    "    static_sample_embeddings = convert_tokens_to_embeddings(sample_tokens, glove_embeddings, EMBEDDING_DIM)\n",
    "    print(\"Static Embeddings for Sample Sentence:\")\n",
    "    for token, emb in zip(sample_tokens, static_sample_embeddings):\n",
    "        print(f\"Token: {token}, Static Embedding: {emb[:6]} ...\")  # Display first 6 values\n",
    "\n",
    "    # Apply to the dataset\n",
    "    static_token_embeddings = convert_tokens_to_embeddings(tokens, glove_embeddings, EMBEDDING_DIM)\n",
    "    print(f\"\\nStatic Embeddings for Dataset (First 5 Tokens):\")\n",
    "    for token, emb in zip(tokens[:5], static_token_embeddings[:5]):\n",
    "        print(f\"Token: {token}, Static Embedding: {emb[:6]} ...\")\n",
    "\n",
    "    # Step 5: Generate positional embeddings\n",
    "    print(\"\\n=== Step 5: Generating Positional Embeddings ===\")\n",
    "    # First, demonstrate with the sample sentence\n",
    "    position_embeddings_sample = positional_encoding(len(sample_tokens), EMBEDDING_DIM)\n",
    "    print(\"Positional Embeddings for Sample Sentence:\")\n",
    "    for pos, emb in enumerate(position_embeddings_sample):\n",
    "        print(f\"Position {pos}: {emb[:5]} ...\")\n",
    "\n",
    "    # Apply to the dataset (positional embeddings for the first few tokens)\n",
    "    position_embeddings_dataset = positional_encoding(len(tokens), EMBEDDING_DIM)\n",
    "    print(f\"\\nPositional Embeddings for Dataset (First 5 Positions):\")\n",
    "    for pos, emb in enumerate(position_embeddings_dataset[:5]):\n",
    "        print(f\"Position {pos}: {emb[:5]} ...\")\n",
    "\n",
    "    # Step 6: Add positional embeddings to static embeddings to create combined embeddings\n",
    "    print(\"\\n=== Step 6: Adding Positional Embeddings to Create Combined Embeddings ===\")\n",
    "    # First, demonstrate with the sample sentence\n",
    "    combined_sample_embeddings = add_positional_embeddings(static_sample_embeddings, EMBEDDING_DIM)\n",
    "    print(\"\\nCombined Embeddings (Static + Positional) for Sample Sentence:\")\n",
    "    for token, emb in zip(sample_tokens, combined_sample_embeddings):\n",
    "        print(f\"Token: {token}, Combined Embedding: {emb[:5]} ...\")\n",
    "\n",
    "    # Apply to the dataset\n",
    "    combined_embeddings = add_positional_embeddings(static_token_embeddings, EMBEDDING_DIM)\n",
    "    print(f\"\\nCombined Embeddings for Dataset (First 5 Tokens):\")\n",
    "    for token, emb in zip(tokens[:5], combined_embeddings[:5]):\n",
    "        print(f\"Token: {token}, Combined Embedding: {emb[:5]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27457d1-09b1-48ca-b89f-5f2b6a759a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
