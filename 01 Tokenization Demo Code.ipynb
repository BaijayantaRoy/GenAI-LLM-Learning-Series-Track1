{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2edf5379-2a78-4b13-8406-100908e63b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['We', 'Love', 'Large', 'Language', 'Model', '(LLM).']\n",
      "Character Tokenization: ['W', 'e', ' ', 'L', 'o', 'v', 'e', ' ', 'L', 'a', 'r', 'g', 'e', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'M', 'o', 'd', 'e', 'l', ' ', '(', 'L', 'L', 'M', ')', '.']\n",
      "Word Tokenization: ['We', 'Love', 'Large', 'Language', 'Model', 'LLM']\n",
      "Sentence Tokenization: ['We Love Large Language Model (LLM).']\n",
      "BPE Tokenization: ['We', 'Love', 'Large', 'Language', 'Model', '(', 'LLM', ').']\n",
      "WordPiece Tokenization: ['we', 'love', 'large', 'language', 'model', '(', 'll', '##m', ')', '.']\n",
      "SentencePiece Tokenization: ['▁', 'W', 'e', '▁L', 'o', 'v', 'e', '▁L', 'a', 'r', 'ge', '▁L', 'a', 'n', 'g', 'u', 'a', 'ge', '▁', 'M', 'o', 'd', 'e', 'l', '▁', '(', 'L', 'L', 'M', ')', '.']\n",
      "Unigram Tokenization: ['W', 'e', 'L', 'o', 'v', 'e', 'La', 'r', 'g', 'e', 'La', 'n', 'g', 'u', 'a', 'g', 'e', 'M', 'o', 'd', 'e', 'l', '(', 'L', 'L', 'M', ')', '.']\n",
      "Byte-Level BPE Tokenization: ['ĠWe', 'ĠLove', 'ĠLarge', 'ĠLanguage', 'ĠModel', 'Ġ(', 'LLM', ').']\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sentence = \"We Love Large Language Model (LLM).\"\n",
    "\n",
    "# ==================================================\n",
    "# 1. Whitespace Tokenization\n",
    "# Splits text based on whitespace (spaces, tabs, newlines)\n",
    "# ==================================================\n",
    "def whitespace_tokenization(text):\n",
    "    return text.split()\n",
    "\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 2. Character Tokenization\n",
    "# Splits text into individual characters\n",
    "# ==================================================\n",
    "def character_tokenization(text):\n",
    "    return list(text)\n",
    "\n",
    "print(\"Character Tokenization:\", character_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 3. Word Tokenization\n",
    "# Splits text into words using a simple regex-based tokenizer\n",
    "# ==================================================\n",
    "import re\n",
    "\n",
    "def word_tokenization(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"Word Tokenization:\", word_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 4. Sentence Tokenization\n",
    "# Splits text into sentences using NLTK\n",
    "# ==================================================\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_tokenization(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokenization:\", sentence_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 5. Byte-Pair Encoding (BPE)\n",
    "# Uses Hugging Face's `tokenizers` library for BPE\n",
    "# ==================================================\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize and train a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train_from_iterator([sentence], trainer)\n",
    "\n",
    "def bpe_tokenization(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "print(\"BPE Tokenization:\", bpe_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 6. WordPiece Tokenization\n",
    "# Uses Hugging Face's `transformers` library for WordPiece\n",
    "# ==================================================\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load a pre-trained WordPiece tokenizer\n",
    "wordpiece_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def wordpiece_tokenization(text):\n",
    "    return wordpiece_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"WordPiece Tokenization:\", wordpiece_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 7. SentencePiece Tokenization\n",
    "# Uses the `sentencepiece` library\n",
    "# ==================================================\n",
    "import sentencepiece as spm\n",
    "import io\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"We Love Large Language Model (LLM).\"\n",
    "\n",
    "# Create an in-memory file-like object with the sample sentence\n",
    "text_data = io.StringIO(sentence)\n",
    "\n",
    "# Train SentencePiece model in-memory\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=text_data,  # Use the in-memory data\n",
    "    model_prefix=\"spm_model\",     # Prefix for the model files\n",
    "    vocab_size=23,                # Vocabulary size\n",
    "    model_type=\"bpe\",         # Model type (unigram or bpe)\n",
    "    input_sentence_size=-1,       # Number of sentences to use for training\n",
    "    pad_id=0,                     # Padding token ID\n",
    "    unk_id=1,                     # Unknown token ID\n",
    "    bos_id=2,                     # Beginning-of-sentence token ID\n",
    "    eos_id=3                      # End-of-sentence token ID\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_model.model\")\n",
    "\n",
    "# Tokenize the sentence\n",
    "def sentencepiece_tokenization(text):\n",
    "    return sp.encode_as_pieces(text)\n",
    "\n",
    "print(\"SentencePiece Tokenization:\", sentencepiece_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 8. Unigram Tokenization\n",
    "# Uses Hugging Face's `tokenizers` library for Unigram\n",
    "# ==================================================\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize and train a Unigram tokenizer\n",
    "tokenizer = Tokenizer(Unigram())\n",
    "trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train_from_iterator([sentence], trainer)\n",
    "\n",
    "def unigram_tokenization(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "print(\"Unigram Tokenization:\", unigram_tokenization(sentence))\n",
    "\n",
    "# ==================================================\n",
    "# 9. Byte-Level BPE (BBPE)\n",
    "# Uses Hugging Face's `tokenizers` library for BBPE\n",
    "# ==================================================\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# Initialize and train a Byte-Level BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.train_from_iterator([sentence], trainer)\n",
    "\n",
    "def bbpe_tokenization(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "print(\"Byte-Level BPE Tokenization:\", bbpe_tokenization(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db441dc-2f3c-4365-95c2-219a63396d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
